#!/usr/bin/env python3
"""
HBaseæ•°æ®æ¸…æ´—å·¥å…· - ç®€åŒ–ç‰ˆ
çº¯æ•°æ®æ¸…æ´—å’Œåˆ†æå·¥å…·ï¼Œä¸æä¾›ä»»ä½•æ¨èæˆ–å»ºè®®
"""
import json
import os
import re
import gzip
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics
from typing import Dict, List, Any, Optional
from strands import tool

@tool
def analyze_hbase_logs(
    log_dir: str = "hbase_log",
    start_time: Optional[str] = None,
    end_time: Optional[str] = None,
    focus_areas: Optional[List[str]] = None,
    target_nodes: Optional[List[str]] = None
) -> dict:
    """
    æ¸…æ´—å’Œè§£æHBaseæ—¥å¿—æ–‡ä»¶æ•°æ®ã€‚çº¯æ•°æ®å¤„ç†ï¼Œä¸æä¾›æ¨èæˆ–å»ºè®®ã€‚
    
    Args:
        log_dir: HBaseæ—¥å¿—ç›®å½•è·¯å¾„
        start_time: åˆ†æå¼€å§‹æ—¶é—´ (æ ¼å¼: "2025-09-12 16:00:00")
        end_time: åˆ†æç»“æŸæ—¶é—´ (æ ¼å¼: "2025-09-12 18:00:00")
        focus_areas: å…³æ³¨é¢†åŸŸ (å¯é€‰ï¼Œå¦‚: ["handler", "wal", "gc", "memory", "compaction", "split", "flush", "queue", "network", "table", "balancer", "errors", "clients", "performance"])
        target_nodes: ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨ (å¯é€‰ï¼Œå¦‚: ["ip-10-25-130-219"])
        
    Returns:
        dict: æ¸…æ´—åçš„æ—¥å¿—æ•°æ®ï¼ŒåŒ…å«äº‹ä»¶ç»Ÿè®¡ã€é”™è¯¯ç»Ÿè®¡ã€æ€§èƒ½æ•°æ®ç­‰
    """
    print("ğŸ” å¼€å§‹åˆ†æHBaseæ—¥å¿—...")
    print(f"ğŸ“‚ æ—¥å¿—ç›®å½•: {log_dir}")
    if target_nodes:
        print(f"ğŸ¯ ç›®æ ‡èŠ‚ç‚¹: {target_nodes}")
    if focus_areas:
        print(f"ğŸ” å…³æ³¨é¢†åŸŸ: {focus_areas}")
    
    # 1. å‘ç°æ—¥å¿—æ–‡ä»¶
    print("\nğŸ“ æ­¥éª¤1: å‘ç°æ—¥å¿—æ–‡ä»¶...")
    log_files = _discover_log_files(log_dir)
    print(f"   âœ“ å‘ç° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
    
    # 2. ç¡®å®šæ—¶é—´çª—å£
    print("\nâ° æ­¥éª¤2: è§£ææ—¶é—´çª—å£...")
    time_window = _parse_time_window(start_time, end_time)
    duration = time_window['end'] - time_window['start']
    print(f"   âœ“ åˆ†ææ—¶é—´çª—å£: {time_window['start_str']} - {time_window['end_str']}")
    print(f"   âœ“ æ—¶é—´è·¨åº¦: {duration}")
    
    # 3. è¿‡æ»¤ç›¸å…³æ–‡ä»¶
    print("\nï¿½ æ­¥éª¤ 3: è¿‡æ»¤ç›¸å…³æ–‡ä»¶...")
    relevant_files = _filter_relevant_files(log_files, time_window, target_nodes)
    print(f"   âœ“ ç­›é€‰å‡º {len(relevant_files)} ä¸ªç›¸å…³æ–‡ä»¶ (ä» {len(log_files)} ä¸ªæ–‡ä»¶ä¸­)")
    
    # 4. è§£ææ—¥å¿—
    print("\nğŸ“ æ­¥éª¤4: è§£ææ—¥å¿—å†…å®¹...")
    log_data = _parse_log_files(relevant_files, time_window, focus_areas)
    print(f"   âœ“ è§£æäº† {log_data['total_entries']} æ¡æ—¥å¿—è®°å½•")
    print(f"   âœ“ æ¶‰åŠèŠ‚ç‚¹: {log_data['nodes']} ({len(log_data['nodes'])} ä¸ª)")
    
    # 5. æ•°æ®ç»Ÿè®¡å’Œåˆ†ç±»
    print("\nğŸ“Š æ­¥éª¤5: æ•°æ®ç»Ÿè®¡å’Œåˆ†ç±»...")
    analysis_result = _analyze_parsed_logs(log_data)
    print(f"   âœ“ ç»Ÿè®¡åˆ° {len(analysis_result['anomalies'])} ä¸ªæ•°æ®å¼‚å¸¸")
    print(f"   âœ“ è¯†åˆ«åˆ° {len(analysis_result['performance_issues'])} ä¸ªæ€§èƒ½æ•°æ®")
    print(f"   âœ“ è®°å½• {len(log_data['errors'])} ä¸ªé”™è¯¯")
    
    print("\nâœ… æ—¥å¿—åˆ†æå®Œæˆ!")
    
    return {
        'analysis_window': time_window,
        'files_processed': len(relevant_files),
        'total_entries': log_data['total_entries'],
        'events_summary': analysis_result['events_summary'],
        'anomalies': analysis_result['anomalies'],
        'performance_issues': analysis_result['performance_issues'],
        'error_summary': analysis_result['error_summary'],
        'summary': analysis_result['summary']
    }

@tool
def analyze_hbase_metrics(
    metrics_dir: str = "hbase_metrics",
    target_time: Optional[str] = None,
    hours_range: int = 72,  # é»˜è®¤3å¤©
    metric_types: Optional[List[str]] = None
) -> dict:
    """
    æ¸…æ´—å’Œå¤„ç†HBaseæ€§èƒ½æŒ‡æ ‡æ•°æ®ã€‚çº¯æ•°æ®å¤„ç†ï¼Œä¸æä¾›æ¨èæˆ–å»ºè®®ã€‚
    
    Args:
        metrics_dir: æŒ‡æ ‡æ•°æ®ç›®å½•è·¯å¾„
        target_time: ç›®æ ‡æ—¶é—´ç‚¹ (å¯é€‰ï¼Œæ ¼å¼: "2025-09-12 16:00:00"ï¼ŒæœªæŒ‡å®šåˆ™åˆ†ææœ€è¿‘3å¤©)
        hours_range: åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼Œé»˜è®¤72å°æ—¶å³3å¤©ï¼‰
        metric_types: æŒ‡æ ‡ç±»å‹ (å¯é€‰ï¼Œå¦‚: ["handler", "wal", "queue", "gc"])
        
    Returns:
        dict: æ¸…æ´—åçš„æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯ã€å¼‚å¸¸æ£€æµ‹ç»“æœã€è¶‹åŠ¿æ•°æ®ç­‰
    """
    print("ğŸ“ˆ å¼€å§‹åˆ†æHBaseæ€§èƒ½æŒ‡æ ‡...")
    
    # 1. åŠ è½½æŒ‡æ ‡æ•°æ®
    metrics_data = _load_metrics_data(metrics_dir, target_time, hours_range)
    print(f"ğŸ“Š åŠ è½½äº† {len(metrics_data)} ä¸ªèŠ‚ç‚¹çš„æŒ‡æ ‡æ•°æ®")
    
    # 2. æŒ‡æ ‡åˆ†æ
    metrics_analysis = _analyze_metrics_comprehensive(metrics_data, metric_types)
    print(f"ğŸ” åˆ†æäº† {len(metrics_analysis['node_analysis'])} ä¸ªèŠ‚ç‚¹")
    
    # 3. æ•°æ®å¼‚å¸¸ç»Ÿè®¡
    anomalies = _detect_metrics_anomalies_comprehensive(metrics_data, metrics_analysis)
    print(f"ğŸ“Š ç»Ÿè®¡åˆ° {len(anomalies)} ä¸ªæŒ‡æ ‡å¼‚å¸¸")
    
    # 4. æ€§èƒ½æ•°æ®åˆ†ç±»
    bottlenecks = _identify_performance_bottlenecks(metrics_analysis, anomalies)
    
    return {
        'analysis_window': {
            'target_time': target_time,
            'hours_range': hours_range
        },
        'metrics_summary': metrics_analysis,
        'anomalies': anomalies,
        'performance_bottlenecks': bottlenecks,
        'trends': _analyze_performance_trends(metrics_data),
        'summary': _generate_metrics_summary(metrics_analysis, anomalies)
    }

# ============================================================================
# æ—¥å¿—åˆ†æè¾…åŠ©å‡½æ•°
# ============================================================================

def _discover_log_files(log_dir: str) -> List[str]:
    """å‘ç°æ—¥å¿—æ–‡ä»¶"""
    log_files = []
    
    if not os.path.exists(log_dir):
        print(f"âš ï¸ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
        return log_files
    
    for root, dirs, files in os.walk(log_dir):
        for file in files:
            # åŒ…å«æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ï¼š.log, .out, .gz, ä»¥åŠå¸¦æ—¥æœŸçš„æ–‡ä»¶
            if (file.endswith(('.log', '.out', '.gz')) or 
                re.search(r'\d{4}-\d{2}-\d{2}-\d{2}', file)):
                file_path = os.path.join(root, file)
                log_files.append(file_path)
    
    return log_files

def _parse_time_window(start_time: Optional[str], end_time: Optional[str]) -> dict:
    """è§£ææ—¶é—´çª—å£"""
    if start_time and end_time:
        start_dt = datetime.strptime(start_time, "%Y-%m-%d %H:%M:%S")
        end_dt = datetime.strptime(end_time, "%Y-%m-%d %H:%M:%S")
    elif start_time:
        start_dt = datetime.strptime(start_time, "%Y-%m-%d %H:%M:%S")
        end_dt = start_dt + timedelta(hours=1)  # é»˜è®¤1å°æ—¶çª—å£
    else:
        # é»˜è®¤åˆ†ææœ€è¿‘1å°æ—¶
        end_dt = datetime.now()
        start_dt = end_dt - timedelta(hours=1)
    
    return {
        'start': start_dt,
        'end': end_dt,
        'start_str': start_dt.strftime("%Y-%m-%d %H:%M:%S"),
        'end_str': end_dt.strftime("%Y-%m-%d %H:%M:%S")
    }

def _filter_relevant_files(log_files: List[str], time_window: dict, target_nodes: Optional[List[str]]) -> List[str]:
    """è¿‡æ»¤ç›¸å…³çš„æ—¥å¿—æ–‡ä»¶"""
    relevant_files = []
    
    # æå–æ—¶é—´çª—å£çš„æ—¥æœŸ
    target_dates = _extract_target_dates(time_window)
    print(f"   ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_dates}")
    
    # ç»Ÿè®¡èŠ‚ç‚¹åˆ†å¸ƒ
    all_nodes = set()
    date_filtered_files = []
    
    for log_file in log_files:
        node_name = _extract_node_name(log_file)
        all_nodes.add(node_name)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«ç›®æ ‡æ—¥æœŸ
        if _is_file_relevant_for_dates(log_file, target_dates):
            date_filtered_files.append(log_file)
    
    print(f"   ğŸŒ å‘ç°èŠ‚ç‚¹: {sorted(list(all_nodes))} ({len(all_nodes)} ä¸ª)")
    print(f"   ğŸ“… æ—¥æœŸè¿‡æ»¤å: {len(date_filtered_files)} ä¸ªæ–‡ä»¶")
    
    # èŠ‚ç‚¹è¿‡æ»¤
    if target_nodes:
        print(f"   ğŸ¯ åº”ç”¨èŠ‚ç‚¹è¿‡æ»¤: {target_nodes}")
        for log_file in date_filtered_files:
            node_name = _extract_node_name(log_file)
            if node_name in target_nodes:
                relevant_files.append(log_file)
        print(f"   ğŸ¯ èŠ‚ç‚¹è¿‡æ»¤å: {len(relevant_files)} ä¸ªæ–‡ä»¶")
    else:
        relevant_files = date_filtered_files
        print(f"   âœ“ æ— èŠ‚ç‚¹è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰ç›¸å…³æ–‡ä»¶")
    
    return relevant_files

def _extract_target_dates(time_window: dict) -> List[str]:
    """ä»æ—¶é—´çª—å£æå–ç›®æ ‡æ—¥æœŸåˆ—è¡¨"""
    start_date = time_window['start'].date()
    end_date = time_window['end'].date()
    
    dates = []
    current_date = start_date
    
    while current_date <= end_date:
        dates.append(current_date.strftime('%Y-%m-%d'))
        current_date = datetime.combine(current_date, datetime.min.time()).date() + timedelta(days=1)
    
    return dates

def _is_file_relevant_for_dates(file_path: str, target_dates: List[str]) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ç›®æ ‡æ—¥æœŸç›¸å…³"""
    filename = os.path.basename(file_path)
    
    # æ£€æŸ¥æ–‡ä»¶åä¸­æ˜¯å¦åŒ…å«ç›®æ ‡æ—¥æœŸ
    for date_str in target_dates:
        if date_str in filename:
            return True
    
    # å¯¹äºå½“å‰æ—¥å¿—æ–‡ä»¶ï¼ˆæ²¡æœ‰æ—¥æœŸåç¼€çš„.logæ–‡ä»¶ï¼‰ï¼Œä¹ŸåŒ…å«è¿›æ¥
    if filename.endswith('.log') and not re.search(r'\d{4}-\d{2}-\d{2}', filename):
        return True
    
    # å¯¹äº.outæ–‡ä»¶ï¼Œä¹Ÿæ£€æŸ¥æ—¥æœŸ
    if filename.endswith('.out'):
        return any(date_str in filename for date_str in target_dates)
    
    return False

def _extract_node_name(file_path: str) -> str:
    """ä»æ–‡ä»¶è·¯å¾„æå–èŠ‚ç‚¹å"""
    filename = os.path.basename(file_path)
    
    # åŒ¹é… ip-xxx-xxx-xxx-xxx æ ¼å¼ï¼ˆæ›´å¹¿æ³›çš„IPæ ¼å¼ï¼‰
    ip_match = re.search(r'ip-\d+-\d+-\d+-\d+', filename)
    if ip_match:
        return ip_match.group()
    
    # å…¶ä»–æ ¼å¼çš„èŠ‚ç‚¹åæå–
    return filename.replace('.log', '').replace('.out', '')

def _parse_log_files(log_files: List[str], time_window: dict, focus_areas: Optional[List[str]]) -> dict:
    """è§£ææ—¥å¿—æ–‡ä»¶"""
    log_data = {
        'total_entries': 0,
        'events': [],
        'errors': [],
        'nodes': set()
    }
    
    timestamp_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d{3}')
    processed_files = 0
    failed_files = 0
    
    if focus_areas:
        print(f"   ğŸ” å…³æ³¨é¢†åŸŸ: {focus_areas}")
    else:
        print(f"   ğŸ” åˆ†ææ‰€æœ‰é¢†åŸŸ: handler, wal, gc, memory, compaction, split, flush, queue, network, table, balancer, errors, clients, performance")
    
    for i, log_file in enumerate(log_files):
        node_name = _extract_node_name(log_file)
        log_data['nodes'].add(node_name)
        file_entries = 0
        
        try:
            file_opener = gzip.open if log_file.endswith('.gz') else open
            mode = 'rt' if log_file.endswith('.gz') else 'r'
            
            with file_opener(log_file, mode, encoding='utf-8', errors='ignore') as f:
                for line in f:
                    # ç¡®ä¿lineæ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if isinstance(line, bytes):
                        line = line.decode('utf-8', errors='ignore')
                    
                    # æå–æ—¶é—´æˆ³
                    timestamp_match = timestamp_pattern.search(line)
                    if not timestamp_match:
                        continue
                    
                    try:
                        log_time = datetime.strptime(timestamp_match.group(1), "%Y-%m-%d %H:%M:%S")
                        if not (time_window['start'] <= log_time <= time_window['end']):
                            continue
                    except:
                        continue
                    
                    log_data['total_entries'] += 1
                    file_entries += 1
                    
                    # è§£ææ—¥å¿—è¡Œ
                    _parse_log_line(line, log_time, node_name, log_data, focus_areas)
            
            processed_files += 1
            if file_entries > 0:
                print(f"   ğŸ“„ {node_name}: {file_entries} æ¡è®°å½• ({os.path.basename(log_file)})")
                    
        except Exception as e:
            failed_files += 1
            print(f"   âš ï¸ å¤„ç†å¤±è´¥ {os.path.basename(log_file)}: {e}")
    
    log_data['nodes'] = list(log_data['nodes'])
    print(f"   âœ“ æˆåŠŸå¤„ç†: {processed_files} ä¸ªæ–‡ä»¶")
    if failed_files > 0:
        print(f"   âš ï¸ å¤„ç†å¤±è´¥: {failed_files} ä¸ªæ–‡ä»¶")
    
    return log_data

def _parse_log_line(line: str, log_time: datetime, node_name: str, log_data: dict, focus_areas: Optional[List[str]]):
    """è§£æå•è¡Œæ—¥å¿—"""
    # Handlerç›¸å…³
    if not focus_areas or 'handler' in focus_areas:
        handler_match = re.search(r'handler=(\d+)', line)
        if handler_match:
            handler_count = int(handler_match.group(1))
            log_data['events'].append({
                'type': 'handler_usage',
                'timestamp': log_time,
                'node': node_name,
                'value': handler_count,
                'line': line.strip()
            })
    
    # WALç›¸å…³
    if not focus_areas or 'wal' in focus_areas:
        # WALæ…¢åŒæ­¥
        wal_match = re.search(r'Slow sync cost: (\d+) ms', line)
        if wal_match:
            sync_time = int(wal_match.group(1))
            log_data['events'].append({
                'type': 'wal_slow_sync',
                'timestamp': log_time,
                'node': node_name,
                'value': sync_time,
                'line': line.strip()
            })
        
        # WALæ»šåŠ¨
        if 'Rolling' in line and 'WAL' in line:
            log_data['events'].append({
                'type': 'wal_rolling',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # WALå¤§å°
        wal_size_match = re.search(r'WAL.*size=(\d+)', line)
        if wal_size_match:
            wal_size = int(wal_size_match.group(1))
            log_data['events'].append({
                'type': 'wal_size',
                'timestamp': log_time,
                'node': node_name,
                'value': wal_size,
                'line': line.strip()
            })
    
    # GCç›¸å…³
    if not focus_areas or 'gc' in focus_areas:
        # GCæš‚åœ
        gc_pause_match = re.search(r'GC pause (\d+)ms', line)
        if gc_pause_match:
            pause_time = int(gc_pause_match.group(1))
            log_data['events'].append({
                'type': 'gc_pause',
                'timestamp': log_time,
                'node': node_name,
                'value': pause_time,
                'line': line.strip()
            })
        
        # GCä¿¡æ¯
        if 'GC' in line and any(gc_type in line for gc_type in ['ParNew', 'CMS', 'G1', 'Full GC']):
            log_data['events'].append({
                'type': 'gc_event',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # å†…å­˜ç›¸å…³
    if not focus_areas or 'memory' in focus_areas:
        # å †å†…å­˜ä½¿ç”¨
        heap_match = re.search(r'heap.*?(\d+)M/(\d+)M', line)
        if heap_match:
            used_heap = int(heap_match.group(1))
            total_heap = int(heap_match.group(2))
            log_data['events'].append({
                'type': 'heap_usage',
                'timestamp': log_time,
                'node': node_name,
                'used': used_heap,
                'total': total_heap,
                'line': line.strip()
            })
        
        # å†…å­˜ä¸è¶³è­¦å‘Š
        if 'OutOfMemoryError' in line or 'low memory' in line.lower():
            log_data['events'].append({
                'type': 'memory_warning',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # å‹ç¼©ç›¸å…³
    if not focus_areas or 'compaction' in focus_areas:
        # å‹ç¼©å¼€å§‹
        if 'Starting compaction' in line:
            log_data['events'].append({
                'type': 'compaction_start',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # å‹ç¼©å®Œæˆ
        compaction_match = re.search(r'Completed compaction.*?(\d+)ms', line)
        if compaction_match:
            compaction_time = int(compaction_match.group(1))
            log_data['events'].append({
                'type': 'compaction_complete',
                'timestamp': log_time,
                'node': node_name,
                'value': compaction_time,
                'line': line.strip()
            })
        
        # ä¸»å‹ç¼©
        if 'major compaction' in line.lower():
            log_data['events'].append({
                'type': 'major_compaction',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # åˆ†è£‚ç›¸å…³
    if not focus_areas or 'split' in focus_areas:
        # Regionåˆ†è£‚
        if 'Splitting' in line and 'region' in line.lower():
            log_data['events'].append({
                'type': 'region_split',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # åˆ†è£‚å®Œæˆ
        if 'Split' in line and 'completed' in line.lower():
            log_data['events'].append({
                'type': 'split_complete',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # åˆ·å†™ç›¸å…³
    if not focus_areas or 'flush' in focus_areas:
        # åˆ·å†™å¼€å§‹
        if 'Flushing' in line:
            log_data['events'].append({
                'type': 'flush_start',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # åˆ·å†™å®Œæˆ
        flush_match = re.search(r'Flushed.*?(\d+)ms', line)
        if flush_match:
            flush_time = int(flush_match.group(1))
            log_data['events'].append({
                'type': 'flush_complete',
                'timestamp': log_time,
                'node': node_name,
                'value': flush_time,
                'line': line.strip()
            })
    
    # é˜Ÿåˆ—ç›¸å…³
    if not focus_areas or 'queue' in focus_areas:
        # RPCé˜Ÿåˆ—
        queue_match = re.search(r'queue=(\d+)', line)
        if queue_match:
            queue_size = int(queue_match.group(1))
            log_data['events'].append({
                'type': 'queue_size',
                'timestamp': log_time,
                'node': node_name,
                'value': queue_size,
                'line': line.strip()
            })
        
        # é˜Ÿåˆ—æ»¡
        if 'queue full' in line.lower() or 'queue overflow' in line.lower():
            log_data['events'].append({
                'type': 'queue_full',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # ç½‘ç»œç›¸å…³
    if not focus_areas or 'network' in focus_areas:
        # ç½‘ç»œè¶…æ—¶
        if 'SocketTimeoutException' in line or 'Connection timeout' in line:
            log_data['events'].append({
                'type': 'network_timeout',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # è¿æ¥é‡ç½®
        if 'Connection reset' in line or 'Connection refused' in line:
            log_data['events'].append({
                'type': 'connection_reset',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # è¡¨æ“ä½œç›¸å…³
    if not focus_areas or 'table' in focus_areas:
        # è¡¨åˆ›å»º
        if 'Creating table' in line:
            log_data['events'].append({
                'type': 'table_create',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # è¡¨åˆ é™¤
        if 'Deleting table' in line:
            log_data['events'].append({
                'type': 'table_delete',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # è¡¨è®¿é—®
        table_match = re.search(r'table=([^,\s]+)', line)
        if table_match:
            table_name = table_match.group(1)
            log_data['events'].append({
                'type': 'table_access',
                'timestamp': log_time,
                'node': node_name,
                'table': table_name,
                'line': line.strip()
            })
    
    # è´Ÿè½½å‡è¡¡ç›¸å…³
    if not focus_areas or 'balancer' in focus_areas:
        # è´Ÿè½½å‡è¡¡å¼€å§‹
        if 'Balancer' in line and 'start' in line.lower():
            log_data['events'].append({
                'type': 'balancer_start',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
        
        # Regionç§»åŠ¨
        if 'Moving region' in line:
            log_data['events'].append({
                'type': 'region_move',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # é”™è¯¯ç›¸å…³
    if not focus_areas or 'errors' in focus_areas:
        if any(keyword in line for keyword in ['ERROR', 'Exception', 'timed out', 'FATAL', 'WARN']):
            log_data['errors'].append({
                'timestamp': log_time,
                'node': node_name,
                'type': _classify_error(line),
                'line': line.strip()
            })
    
    # å®¢æˆ·ç«¯è¿æ¥
    if not focus_areas or 'clients' in focus_areas:
        client_match = re.search(r'connection: ([\d\.]+):\d+', line)
        if client_match:
            client_ip = client_match.group(1)
            log_data['events'].append({
                'type': 'client_connection',
                'timestamp': log_time,
                'node': node_name,
                'client_ip': client_ip,
                'line': line.strip()
            })
        
        # å®¢æˆ·ç«¯æ–­å¼€
        if 'client disconnect' in line.lower() or 'connection closed' in line.lower():
            log_data['events'].append({
                'type': 'client_disconnect',
                'timestamp': log_time,
                'node': node_name,
                'line': line.strip()
            })
    
    # æ€§èƒ½ç›¸å…³
    if not focus_areas or 'performance' in focus_areas:
        # æ…¢æŸ¥è¯¢
        slow_query_match = re.search(r'Slow query.*?(\d+)ms', line)
        if slow_query_match:
            query_time = int(slow_query_match.group(1))
            log_data['events'].append({
                'type': 'slow_query',
                'timestamp': log_time,
                'node': node_name,
                'value': query_time,
                'line': line.strip()
            })
        
        # å“åº”æ—¶é—´
        response_match = re.search(r'response time.*?(\d+)ms', line)
        if response_match:
            response_time = int(response_match.group(1))
            log_data['events'].append({
                'type': 'response_time',
                'timestamp': log_time,
                'node': node_name,
                'value': response_time,
                'line': line.strip()
            })

def _classify_error(line: str) -> str:
    """åˆ†ç±»é”™è¯¯ç±»å‹"""
    line_lower = line.lower()
    
    # è¶…æ—¶ç›¸å…³
    if 'timed out' in line or 'timeout' in line_lower:
        return 'timeout'
    
    # å†…å­˜ç›¸å…³
    elif 'outofmemoryerror' in line_lower or 'out of memory' in line_lower:
        return 'memory'
    
    # ç½‘ç»œç›¸å…³
    elif any(net_err in line_lower for net_err in ['connection', 'socket', 'network']):
        return 'network'
    
    # IOç›¸å…³
    elif any(io_err in line_lower for io_err in ['ioexception', 'disk', 'file']):
        return 'io'
    
    # æƒé™ç›¸å…³
    elif any(perm_err in line_lower for perm_err in ['permission', 'access denied', 'unauthorized']):
        return 'permission'
    
    # é…ç½®ç›¸å…³
    elif any(conf_err in line_lower for conf_err in ['configuration', 'config', 'property']):
        return 'configuration'
    
    # æ•°æ®ç›¸å…³
    elif any(data_err in line_lower for data_err in ['corrupt', 'checksum', 'data']):
        return 'data'
    
    # èµ„æºç›¸å…³
    elif any(res_err in line_lower for res_err in ['resource', 'quota', 'limit']):
        return 'resource'
    
    # çº§åˆ«åˆ†ç±»
    elif 'FATAL' in line:
        return 'fatal'
    elif 'ERROR' in line:
        return 'error'
    elif 'WARN' in line:
        return 'warning'
    elif 'Exception' in line:
        return 'exception'
    else:
        return 'unknown'

def _analyze_parsed_logs(log_data: dict) -> dict:
    """åˆ†æè§£æåçš„æ—¥å¿—æ•°æ®"""
    events = log_data['events']
    errors = log_data['errors']
    
    print(f"   ğŸ“Š åˆ†æ {len(events)} ä¸ªäº‹ä»¶å’Œ {len(errors)} ä¸ªé”™è¯¯...")
    
    # ç»Ÿè®¡Handlerä½¿ç”¨æƒ…å†µ
    handler_events = [e for e in events if e['type'] == 'handler_usage']
    handler_stats = {}
    
    for event in handler_events:
        node = event['node']
        if node not in handler_stats:
            handler_stats[node] = []
        handler_stats[node].append(event['value'])
    
    if handler_events:
        print(f"   ğŸ”§ Handleräº‹ä»¶: {len(handler_events)} ä¸ªï¼Œæ¶‰åŠ {len(handler_stats)} ä¸ªèŠ‚ç‚¹")
        for node, values in handler_stats.items():
            if values:
                max_val = max(values)
                avg_val = statistics.mean(values)
                print(f"      - {node}: æœ€å¤§{max_val}, å¹³å‡{avg_val:.1f} ({len(values)}æ¬¡è®°å½•)")
    
    # æ£€æµ‹å¼‚å¸¸
    anomalies = []
    performance_issues = []
    
    print(f"   ğŸ” å¼€å§‹æ•°æ®å¼‚å¸¸ç»Ÿè®¡...")
    
    # Handleræ•°æ®å¼‚å¸¸ç»Ÿè®¡
    handler_anomalies = 0
    for node, values in handler_stats.items():
        if values:
            max_handler = max(values)
            avg_handler = statistics.mean(values)
            
            if max_handler >= 58:  # æ¥è¿‘é¥±å’Œ
                handler_anomalies += 1
                severity = 'critical' if max_handler >= 60 else 'high'
                anomalies.append({
                    'type': 'handler_saturation',
                    'node': node,
                    'severity': severity,
                    'max_value': max_handler,
                    'avg_value': avg_handler,
                    'description': f'èŠ‚ç‚¹ {node} Handleræ•°æ®: æœ€å¤§{max_handler}, å¹³å‡{avg_handler:.1f}'
                })
                print(f"      ï¿½ Handdleræ•°æ®å¼‚å¸¸: {node} (æœ€å¤§{max_handler}, çº§åˆ«:{severity})")
    
    # WALæ€§èƒ½é—®é¢˜
    wal_events = [e for e in events if e['type'] == 'wal_slow_sync']
    if wal_events:
        slow_syncs = len(wal_events)
        avg_sync_time = statistics.mean([e['value'] for e in wal_events])
        print(f"   â±ï¸ WALæ…¢åŒæ­¥: {slow_syncs} æ¬¡ï¼Œå¹³å‡è€—æ—¶ {avg_sync_time:.1f}ms")
        
        if avg_sync_time > 100:  # è¶…è¿‡100msè®¤ä¸ºæ˜¯æ…¢åŒæ­¥
            performance_issues.append({
                'type': 'wal_performance',
                'severity': 'high',
                'slow_sync_count': slow_syncs,
                'avg_sync_time': avg_sync_time,
                'description': f'WALåŒæ­¥æ•°æ®: {slow_syncs}æ¬¡æ…¢åŒæ­¥, å¹³å‡{avg_sync_time:.1f}ms'
            })
            print(f"      ï¿½ WALåŒæ­¥æ•°æ®:: å¹³å‡åŒæ­¥æ—¶é—´ ({avg_sync_time:.1f}ms > 100ms)")
    
    # é”™è¯¯ç‡åˆ†æ
    if errors:
        error_rate = len(errors) / log_data['total_entries'] * 100 if log_data['total_entries'] > 0 else 0
        error_types = {}
        for error in errors:
            error_type = error.get('type', 'unknown')
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        print(f"   âŒ é”™è¯¯åˆ†æ: {len(errors)} ä¸ªé”™è¯¯ï¼Œé”™è¯¯ç‡ {error_rate:.2f}%")
        print(f"      é”™è¯¯ç±»å‹åˆ†å¸ƒ: {error_types}")
        
        if error_rate > 5:  # é”™è¯¯ç‡è¶…è¿‡5%
            anomalies.append({
                'type': 'high_error_rate',
                'severity': 'high',
                'error_count': len(errors),
                'error_rate': error_rate,
                'description': f'é”™è¯¯ç»Ÿè®¡: {len(errors)}ä¸ªé”™è¯¯, é”™è¯¯ç‡{error_rate:.1f}%'
            })
            print(f"      ï¿½ é”™è¯¯ç‡ç‡æ•°æ®: {error_rate:.2f}% > 5%")
    
    # äº‹ä»¶æ±‡æ€»
    events_summary = _summarize_events(events)
    
    # é”™è¯¯æ±‡æ€»
    error_summary = _summarize_errors(errors)
    
    # ç”Ÿæˆæ‘˜è¦
    summary = _generate_analysis_summary(log_data, anomalies, performance_issues)
    
    print(f"   âœ… æ•°æ®ç»Ÿè®¡å®Œæˆ: {len(anomalies)} ä¸ªå¼‚å¸¸, {len(performance_issues)} ä¸ªæ€§èƒ½æ•°æ®")
    
    return {
        'events_summary': events_summary,
        'anomalies': anomalies,
        'performance_issues': performance_issues,
        'error_summary': error_summary,
        'handler_stats': handler_stats,
        'summary': summary
    }

def _summarize_events(events: List[dict]) -> dict:
    """æ±‡æ€»äº‹ä»¶ä¿¡æ¯"""
    if not events:
        return {'total': 0, 'types': {}}
    
    # ç»Ÿè®¡äº‹ä»¶ç±»å‹
    event_types = {}
    node_stats = {}
    time_distribution = {}
    
    for event in events:
        event_type = event.get('type', 'unknown')
        node = event.get('node', 'unknown')
        timestamp = event.get('timestamp')
        
        # äº‹ä»¶ç±»å‹ç»Ÿè®¡
        event_types[event_type] = event_types.get(event_type, 0) + 1
        
        # èŠ‚ç‚¹ç»Ÿè®¡
        if node not in node_stats:
            node_stats[node] = {}
        node_stats[node][event_type] = node_stats[node].get(event_type, 0) + 1
        
        # æ—¶é—´åˆ†å¸ƒï¼ˆæŒ‰å°æ—¶ï¼‰
        if timestamp:
            hour_key = timestamp.strftime('%H:00')
            time_distribution[hour_key] = time_distribution.get(hour_key, 0) + 1
    
    return {
        'total': len(events),
        'types': event_types,
        'by_node': node_stats,
        'time_distribution': time_distribution,
        'top_event_types': sorted(event_types.items(), key=lambda x: x[1], reverse=True)[:5]
    }

def _summarize_errors(errors: List[dict]) -> dict:
    """æ±‡æ€»é”™è¯¯ä¿¡æ¯"""
    if not errors:
        return {'total': 0, 'types': {}}
    
    error_types = {}
    node_stats = {}
    
    for error in errors:
        error_type = error.get('type', 'unknown')
        node = error.get('node', 'unknown')
        
        # é”™è¯¯ç±»å‹ç»Ÿè®¡
        error_types[error_type] = error_types.get(error_type, 0) + 1
        
        # èŠ‚ç‚¹ç»Ÿè®¡
        if node not in node_stats:
            node_stats[node] = {}
        node_stats[node][error_type] = node_stats[node].get(error_type, 0) + 1
    
    return {
        'total': len(errors),
        'types': error_types,
        'by_node': node_stats,
        'top_error_types': sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:3]
    }

def _generate_analysis_summary(log_data: dict, anomalies: List[dict], performance_issues: List[dict]) -> str:
    """ç”Ÿæˆåˆ†ææ‘˜è¦"""
    summary_parts = []
    
    summary_parts.append(f"æ—¥å¿—æ¡ç›®: {log_data['total_entries']}")
    summary_parts.append(f"èŠ‚ç‚¹æ•°: {len(log_data['nodes'])}")
    summary_parts.append(f"å¼‚å¸¸: {len(anomalies)}")
    summary_parts.append(f"æ€§èƒ½é—®é¢˜: {len(performance_issues)}")
    summary_parts.append(f"é”™è¯¯: {len(log_data['errors'])}")
    
    return " | ".join(summary_parts)

# ============================================================================
# æŒ‡æ ‡åˆ†æç›¸å…³å‡½æ•°ï¼ˆä¿æŒåŸæœ‰å®ç°ï¼‰
# ============================================================================

def _load_metrics_data(metrics_dir: str, target_time: Optional[str], hours_range: int) -> dict:
    """åŠ è½½æŒ‡æ ‡æ•°æ®"""
    if not os.path.exists(metrics_dir):
        print(f"âš ï¸ æŒ‡æ ‡ç›®å½•ä¸å­˜åœ¨: {metrics_dir}")
        return {}
    
    # ç¡®å®šæ—¶é—´èŒƒå›´
    if target_time:
        start_time = datetime.strptime(target_time, "%Y-%m-%d %H:%M:%S")
        end_time = start_time + timedelta(hours=hours_range)
    else:
        # é»˜è®¤åˆ†ææœ€è¿‘3å¤©
        end_time = datetime.now()
        start_time = end_time - timedelta(days=3)
    
    # åŠ è½½æŒ‡æ ‡æ–‡ä»¶
    metrics_data = {
        'nodes': [],
        'handler_metrics': {},
        'time_range': {
            'start': start_time.strftime("%Y-%m-%d %H:%M:%S"),
            'end': end_time.strftime("%Y-%m-%d %H:%M:%S"),
            'hours': hours_range if target_time else 72
        }
    }
    
    # å°è¯•åŠ è½½download.jsonæ–‡ä»¶
    download_file = os.path.join(metrics_dir, 'download.json')
    if os.path.exists(download_file):
        try:
            with open(download_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è§£ææ—¶åºæ•°æ®
            if 'series' in data:
                for series in data['series']:
                    node_name = _extract_node_from_series_name(series.get('name', ''))
                    if not node_name:
                        continue
                    
                    # æå–æ—¶é—´å’Œæ•°å€¼æ•°æ®
                    time_field = None
                    value_field = None
                    
                    for field in series.get('fields', []):
                        if field.get('type') == 'time':
                            time_field = field
                        elif field.get('type') == 'number':
                            value_field = field
                    
                    if time_field and value_field:
                        timestamps = time_field.get('values', [])
                        values = value_field.get('values', [])
                        
                        # è½¬æ¢æ—¶é—´æˆ³å¹¶è¿‡æ»¤æ—¶é—´èŒƒå›´
                        filtered_data = _filter_metrics_by_time(timestamps, values, start_time, end_time)
                        
                        if filtered_data['timestamps']:
                            metrics_data['nodes'].append(node_name)
                            metrics_data['handler_metrics'][node_name] = filtered_data
            
            print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(metrics_data['nodes'])} ä¸ªèŠ‚ç‚¹çš„æŒ‡æ ‡æ•°æ®")
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æŒ‡æ ‡æ–‡ä»¶å¤±è´¥: {e}")
    
    return metrics_data

def _extract_node_from_series_name(series_name: str) -> Optional[str]:
    """ä»seriesåç§°ä¸­æå–èŠ‚ç‚¹å"""
    # åŒ¹é… "Total: ip-xxx-xxx-xxx-xxx" æ ¼å¼
    match = re.search(r'ip-\d+-\d+-\d+-\d+', series_name)
    return match.group() if match else None

def _filter_metrics_by_time(timestamps: list, values: list, start_time: datetime, end_time: datetime) -> dict:
    """æ ¹æ®æ—¶é—´èŒƒå›´è¿‡æ»¤æŒ‡æ ‡æ•°æ®"""
    filtered_timestamps = []
    filtered_values = []
    
    for i, timestamp_ms in enumerate(timestamps):
        if i >= len(values):
            break
        
        # è½¬æ¢æ¯«ç§’æ—¶é—´æˆ³ä¸ºdatetime
        timestamp_dt = datetime.fromtimestamp(timestamp_ms / 1000)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
        if start_time <= timestamp_dt <= end_time:
            filtered_timestamps.append(timestamp_dt)
            filtered_values.append(values[i])
    
    return {
        'timestamps': filtered_timestamps,
        'values': filtered_values,
        'count': len(filtered_values)
    }

def _analyze_metrics_comprehensive(metrics_data: dict, metric_types: Optional[List[str]]) -> dict:
    """å…¨é¢åˆ†ææŒ‡æ ‡"""
    analysis = {
        'node_analysis': {},
        'cluster_summary': {},
        'performance_metrics': {}
    }
    
    if not metrics_data.get('handler_metrics'):
        return analysis
    
    # åˆ†ææ¯ä¸ªèŠ‚ç‚¹çš„HandleræŒ‡æ ‡
    node_stats = {}
    all_values = []
    
    for node_name, node_data in metrics_data['handler_metrics'].items():
        values = node_data['values']
        timestamps = node_data['timestamps']
        
        if not values:
            continue
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'max': max(values),
            'min': min(values),
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'std': statistics.stdev(values) if len(values) > 1 else 0,
            'count': len(values),
            'timestamps': timestamps,
            'values': values
        }
        
        # æ£€æµ‹é«˜ä½¿ç”¨ç‡æ—¶é—´ç‚¹
        high_usage_points = []
        for i, (ts, val) in enumerate(zip(timestamps, values)):
            if val >= 50:  # Handlerä½¿ç”¨ç‡è¶…è¿‡50è®¤ä¸ºæ˜¯é«˜ä½¿ç”¨
                high_usage_points.append({
                    'timestamp': ts,
                    'value': val,
                    'index': i
                })
        
        stats['high_usage_points'] = high_usage_points
        stats['high_usage_count'] = len(high_usage_points)
        
        # æ£€æµ‹é¥±å’Œæƒ…å†µï¼ˆæ¥è¿‘æˆ–è¾¾åˆ°60ï¼‰
        saturation_points = []
        for i, (ts, val) in enumerate(zip(timestamps, values)):
            if val >= 58:  # æ¥è¿‘é¥±å’Œ
                saturation_points.append({
                    'timestamp': ts,
                    'value': val,
                    'severity': 'critical' if val >= 60 else 'high'
                })
        
        stats['saturation_points'] = saturation_points
        stats['saturation_count'] = len(saturation_points)
        
        node_stats[node_name] = stats
        all_values.extend(values)
    
    analysis['node_analysis'] = node_stats
    
    # é›†ç¾¤çº§åˆ«ç»Ÿè®¡
    if all_values:
        analysis['cluster_summary'] = {
            'total_nodes': len(node_stats),
            'cluster_max': max(all_values),
            'cluster_mean': statistics.mean(all_values),
            'cluster_std': statistics.stdev(all_values) if len(all_values) > 1 else 0,
            'total_data_points': len(all_values)
        }
    
    # æ€§èƒ½æŒ‡æ ‡æ±‡æ€»
    high_usage_nodes = [node for node, stats in node_stats.items() if stats['high_usage_count'] > 0]
    saturated_nodes = [node for node, stats in node_stats.items() if stats['saturation_count'] > 0]
    
    analysis['performance_metrics'] = {
        'high_usage_nodes': high_usage_nodes,
        'saturated_nodes': saturated_nodes,
        'high_usage_node_count': len(high_usage_nodes),
        'saturated_node_count': len(saturated_nodes)
    }
    
    return analysis

def _detect_metrics_anomalies_comprehensive(metrics_data: dict, metrics_analysis: dict) -> List[dict]:
    """ç»Ÿè®¡æŒ‡æ ‡æ•°æ®å¼‚å¸¸"""
    anomalies = []
    
    node_analysis = metrics_analysis.get('node_analysis', {})
    
    for node_name, stats in node_analysis.items():
        # 1. Handleré¥±å’Œå¼‚å¸¸
        if stats['saturation_count'] > 0:
            severity = 'critical' if stats['max'] >= 60 else 'high'
            anomalies.append({
                'type': 'handler_saturation',
                'node': node_name,
                'severity': severity,
                'max_value': stats['max'],
                'saturation_count': stats['saturation_count'],
                'description': f"èŠ‚ç‚¹ {node_name} Handleræ•°æ®: æœ€å¤§å€¼{stats['max']}, {stats['saturation_count']}æ¬¡è¾¾åˆ°é˜ˆå€¼",
                'timestamps': [sp['timestamp'] for sp in stats['saturation_points']],
                'values': [sp['value'] for sp in stats['saturation_points']]
            })
        
        # 2. æŒç»­é«˜ä½¿ç”¨ç‡å¼‚å¸¸
        elif stats['high_usage_count'] > 5:  # è¶…è¿‡5æ¬¡é«˜ä½¿ç”¨ç‡
            anomalies.append({
                'type': 'high_handler_usage',
                'node': node_name,
                'severity': 'medium',
                'max_value': stats['max'],
                'mean_value': stats['mean'],
                'high_usage_count': stats['high_usage_count'],
                'description': f"èŠ‚ç‚¹ {node_name} Handleræ•°æ®: å¹³å‡{stats['mean']:.1f}, {stats['high_usage_count']}æ¬¡è¶…è¿‡50",
                'timestamps': [hp['timestamp'] for hp in stats['high_usage_points']],
                'values': [hp['value'] for hp in stats['high_usage_points']]
            })
        
        # 3. Handlerä½¿ç”¨ç‡æ³¢åŠ¨å¼‚å¸¸
        if stats['std'] > 15 and stats['max'] > 30:  # æ ‡å‡†å·®å¤§ä¸”æœ‰è¾ƒé«˜å³°å€¼
            anomalies.append({
                'type': 'handler_volatility',
                'node': node_name,
                'severity': 'medium',
                'std_value': stats['std'],
                'max_value': stats['max'],
                'description': f"èŠ‚ç‚¹ {node_name} Handleræ³¢åŠ¨æ•°æ®: æ ‡å‡†å·®{stats['std']:.1f}, æœ€å¤§å€¼{stats['max']}"
            })
    
    # 4. é›†ç¾¤çº§åˆ«å¼‚å¸¸
    cluster_summary = metrics_analysis.get('cluster_summary', {})
    performance_metrics = metrics_analysis.get('performance_metrics', {})
    
    if performance_metrics.get('saturated_node_count', 0) > 1:
        anomalies.append({
            'type': 'cluster_saturation',
            'severity': 'critical',
            'affected_nodes': performance_metrics['saturated_nodes'],
            'node_count': performance_metrics['saturated_node_count'],
            'description': f"é›†ç¾¤Handleræ•°æ®: {performance_metrics['saturated_node_count']}ä¸ªèŠ‚ç‚¹è¾¾åˆ°é˜ˆå€¼"
        })
    
    elif performance_metrics.get('high_usage_node_count', 0) > len(node_analysis) * 0.3:  # è¶…è¿‡30%èŠ‚ç‚¹é«˜ä½¿ç”¨ç‡
        anomalies.append({
            'type': 'cluster_high_usage',
            'severity': 'high',
            'affected_nodes': performance_metrics['high_usage_nodes'],
            'node_count': performance_metrics['high_usage_node_count'],
            'description': f"é›†ç¾¤Handleræ•°æ®: {performance_metrics['high_usage_node_count']}ä¸ªèŠ‚ç‚¹é«˜ä½¿ç”¨ç‡"
        })
    
    return anomalies

def _identify_performance_bottlenecks(metrics_analysis: dict, anomalies: List[dict]) -> List[dict]:
    """åˆ†ç±»æ€§èƒ½æ•°æ®"""
    bottlenecks = []
    
    # åŸºäºå¼‚å¸¸æ•°æ®åˆ†ç±»
    critical_anomalies = [a for a in anomalies if a['severity'] == 'critical']
    high_anomalies = [a for a in anomalies if a['severity'] == 'high']
    
    if critical_anomalies:
        bottlenecks.append({
            'type': 'handler_capacity',
            'severity': 'critical',
            'description': 'Handlerå®¹é‡æ•°æ®å¼‚å¸¸',
            'affected_components': [a.get('node', 'cluster') for a in critical_anomalies],
            'impact': 'severe_performance_degradation'
        })
    
    if high_anomalies:
        bottlenecks.append({
            'type': 'resource_pressure',
            'severity': 'high', 
            'description': 'Handlerèµ„æºä½¿ç”¨ç‡é«˜',
            'affected_components': [a.get('node', 'cluster') for a in high_anomalies],
            'impact': 'performance_degradation'
        })
    
    return bottlenecks

def _analyze_performance_trends(metrics_data: dict) -> dict:
    """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
    trends = {}
    
    if not metrics_data.get('handler_metrics'):
        return trends
    
    # åˆ†ææ•´ä½“è¶‹åŠ¿
    all_timestamps = []
    all_values = []
    
    for node_data in metrics_data['handler_metrics'].values():
        all_timestamps.extend(node_data['timestamps'])
        all_values.extend(node_data['values'])
    
    if all_values:
        # æŒ‰æ—¶é—´æ’åº
        time_value_pairs = list(zip(all_timestamps, all_values))
        time_value_pairs.sort(key=lambda x: x[0])
        
        sorted_values = [pair[1] for pair in time_value_pairs]
        
        # è®¡ç®—è¶‹åŠ¿
        if len(sorted_values) >= 10:
            first_half = sorted_values[:len(sorted_values)//2]
            second_half = sorted_values[len(sorted_values)//2:]
            
            first_avg = statistics.mean(first_half)
            second_avg = statistics.mean(second_half)
            
            trend_direction = 'increasing' if second_avg > first_avg * 1.1 else \
                            'decreasing' if second_avg < first_avg * 0.9 else 'stable'
            
            trends = {
                'overall_trend': trend_direction,
                'first_half_avg': first_avg,
                'second_half_avg': second_avg,
                'trend_magnitude': abs(second_avg - first_avg) / first_avg if first_avg > 0 else 0
            }
    
    return trends

def _generate_metrics_summary(metrics_analysis: dict, anomalies: List[dict]) -> str:
    """ç”ŸæˆæŒ‡æ ‡åˆ†ææ‘˜è¦"""
    summary_parts = []
    
    cluster_summary = metrics_analysis.get('cluster_summary', {})
    performance_metrics = metrics_analysis.get('performance_metrics', {})
    
    # åŸºæœ¬ç»Ÿè®¡
    if cluster_summary:
        summary_parts.append(f"èŠ‚ç‚¹æ•°: {cluster_summary.get('total_nodes', 0)}")
        summary_parts.append(f"é›†ç¾¤æœ€å¤§Handler: {cluster_summary.get('cluster_max', 0)}")
        summary_parts.append(f"é›†ç¾¤å¹³å‡Handler: {cluster_summary.get('cluster_mean', 0):.1f}")
    
    # å¼‚å¸¸ç»Ÿè®¡
    critical_count = len([a for a in anomalies if a['severity'] == 'critical'])
    high_count = len([a for a in anomalies if a['severity'] == 'high'])
    medium_count = len([a for a in anomalies if a['severity'] == 'medium'])
    
    if critical_count > 0:
        summary_parts.append(f"ä¸¥é‡å¼‚å¸¸: {critical_count}")
    if high_count > 0:
        summary_parts.append(f"é‡è¦å¼‚å¸¸: {high_count}")
    if medium_count > 0:
        summary_parts.append(f"ä¸­ç­‰å¼‚å¸¸: {medium_count}")
    
    # æ€§èƒ½çŠ¶æ€
    if performance_metrics:
        saturated_count = performance_metrics.get('saturated_node_count', 0)
        high_usage_count = performance_metrics.get('high_usage_node_count', 0)
        
        if saturated_count > 0:
            summary_parts.append(f"é¥±å’ŒèŠ‚ç‚¹: {saturated_count}")
        if high_usage_count > 0:
            summary_parts.append(f"é«˜è´Ÿè½½èŠ‚ç‚¹: {high_usage_count}")
    
    return " | ".join(summary_parts) if summary_parts else "æ— æŒ‡æ ‡æ•°æ®"